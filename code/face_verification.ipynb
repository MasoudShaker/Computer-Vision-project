{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8n6Hd8GYoHY"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "import torchvision.utils\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f7XWXmejD-j"
      },
      "outputs": [],
      "source": [
        "train_set = datasets.LFWPairs(\n",
        "    root='data',\n",
        "    split='train',\n",
        "    transform=ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_set = datasets.LFWPairs(\n",
        "    root='data',\n",
        "    split='test',\n",
        "    transform=ToTensor(),\n",
        "    download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set), len(test_set)"
      ],
      "metadata": {
        "id": "O1sGrCiOUdHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use 20% of training data for validation\n",
        "train_set_size = int(len(train_set) * 0.8)\n",
        "valid_set_size = len(train_set) - train_set_size\n",
        "\n",
        "seed = torch.Generator().manual_seed(42)\n",
        "train_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)"
      ],
      "metadata": {
        "id": "eP-R1GjtWXDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set), len(valid_set), len(test_set)"
      ],
      "metadata": {
        "id": "Ent5pDkcWv3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6pbglhbkr3t"
      },
      "outputs": [],
      "source": [
        "n_samples = 5\n",
        "\n",
        "for i in range(n_samples):\n",
        "  n = 6\n",
        "  m = np.reshape(np.linspace(0,1,n**2), (n,n))\n",
        "  plt.figure(figsize=(14,3))\n",
        "\n",
        "  plt.subplot(141)\n",
        "  plt.imshow(np.transpose(train_set[i][0], (1, 2, 0)))\n",
        "  plt.xticks(range(n))\n",
        "  plt.yticks(range(n))\n",
        "\n",
        "  plt.subplot(142)\n",
        "  plt.imshow(np.transpose(train_set[i][1], (1, 2, 0)))\n",
        "  plt.yticks([])\n",
        "  plt.xticks(range(n))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHsk_MgGn-t8"
      },
      "outputs": [],
      "source": [
        "for i in range(n_samples):\n",
        "  print(train_set[i][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fMfukeEmoOS"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_set,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2,\n",
        "                          batch_size=64)\n",
        "\n",
        "valid_dataloader = DataLoader(valid_set,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2,\n",
        "                          batch_size=64)\n",
        "\n",
        "test_dataloader = DataLoader(test_set,\n",
        "                         shuffle=True,\n",
        "                         num_workers=2,\n",
        "                         batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9nAHrF8nhKA"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "len(batch), len(batch[0]), len(batch[1]), len(batch[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XIQXaxctW2U"
      },
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11,stride=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "            nn.Conv2d(256, 384, kernel_size=3,stride=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(38400, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(256,2)\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        output = self.cnn1(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc1(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "\n",
        "        return output1, output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaKpH8EptX1U"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
        "\n",
        "      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
        "                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "\n",
        "      return loss_contrastive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTPnlD26yNzX"
      },
      "outputs": [],
      "source": [
        "siamese_net = SiameseNetwork().cuda()\n",
        "loss_func = ContrastiveLoss()\n",
        "opt = optim.Adam(siamese_net.parameters(), lr = 0.0003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgyTQpLnySwh"
      },
      "outputs": [],
      "source": [
        "counter = []\n",
        "loss_history = []\n",
        "itr= 0\n",
        "\n",
        "for epoch in range(40):\n",
        "    for i, (first_img, second_img, label) in enumerate(train_dataloader, 0):\n",
        "\n",
        "        first_img, second_img, label = first_img.cuda(), second_img.cuda(), label.cuda()\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        output1, output2 = siamese_net(first_img, second_img)\n",
        "\n",
        "        loss_contrastive = loss_func(output1, output2, label)\n",
        "        loss_contrastive.backward()\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        if i % 100 == 0 :\n",
        "            print(f\"Epoch number {epoch+1}\\nloss: {loss_contrastive.item():.2f}\\n\")\n",
        "            itr += 10\n",
        "\n",
        "            counter.append(itr)\n",
        "            loss_history.append(loss_contrastive.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6ZQAmT135ma"
      },
      "outputs": [],
      "source": [
        "plt.plot(counter, loss_history)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(sum_dist, label, threshold):\n",
        "  if sum_dist < threshold:\n",
        "    prediction = 1\n",
        "  else:\n",
        "    prediction = 0\n",
        "\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "bBOR1F5Zd_Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0vbp6wC1HdH"
      },
      "outputs": [],
      "source": [
        "TP = 0 # True Positive\n",
        "TN = 0 # True Negative\n",
        "FP = 0 # False Postive\n",
        "FN = 0 # False Negative\n",
        "cnt = 0\n",
        "\n",
        "\n",
        "for i, (first_img, second_img, label) in enumerate(valid_dataloader, 0):\n",
        "  batch_size = len(first_img)\n",
        "\n",
        "  emb_vec1, emb_vec2 = siamese_net(first_img.cuda(), second_img.cuda())\n",
        "  euclidean_distance = F.pairwise_distance(emb_vec1, emb_vec2)\n",
        "\n",
        "  for j in range(batch_size):\n",
        "    sum_euclidean = torch.sum(euclidean_distance[j])\n",
        "    prediction = make_prediction(sum_euclidean, label[j], 0.8)\n",
        "\n",
        "    if prediction == label[j]:\n",
        "      if prediction == 1:\n",
        "        TP += 1\n",
        "      else:\n",
        "        TN += 1\n",
        "\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        FP += 1\n",
        "      else:\n",
        "        FN += 1\n",
        "\n",
        "\n",
        "Accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "Precision = TP / (TP+FP)\n",
        "Recall = TP / (TP+FN)\n",
        "Specificity = TN / (TN+FP)\n",
        "F1_Score = 2 * ((Precision*Recall)/(Precision+Recall))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {Accuracy:.2f}\")\n",
        "print(f\"Precision: {Precision:.2f}\")\n",
        "print(f\"Recall: {Recall:.2f}\")\n",
        "print(f\"Specificity: {Specificity:.2f}\")\n",
        "print(f\"F1-Score: {F1_Score:.2f}\")"
      ],
      "metadata": {
        "id": "YwldNSQzMWsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = nn.CosineSimilarity()"
      ],
      "metadata": {
        "id": "JPru0cw3QnfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP = 0 # True Positive\n",
        "TN = 0 # True Negative\n",
        "FP = 0 # False Postive\n",
        "FN = 0 # False Negative\n",
        "cnt = 0\n",
        "\n",
        "\n",
        "for i, (first_img, second_img, label) in enumerate(valid_dataloader, 0):\n",
        "  batch_size = len(first_img)\n",
        "\n",
        "  emb_vec1, emb_vec2 = siamese_net(first_img.cuda(), second_img.cuda())\n",
        "  cosine_similiarity = cosine_sim(emb_vec1, emb_vec2)\n",
        "\n",
        "  for j in range(batch_size):\n",
        "    sum_cosine = torch.sum(cosine_similiarity[j])\n",
        "    prediction = make_prediction(sum_cosine, label[j], 0.9)\n",
        "\n",
        "    if prediction == label[j]:\n",
        "      if prediction == 1:\n",
        "        TP += 1\n",
        "      else:\n",
        "        TN += 1\n",
        "\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        FP += 1\n",
        "      else:\n",
        "        FN += 1\n",
        "\n",
        "\n",
        "Accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "Precision = TP / (TP+FP)\n",
        "Recall = TP / (TP+FN)\n",
        "Specificity = TN / (TN+FP)\n",
        "F1_Score = 2 * ((Precision*Recall)/(Precision+Recall))"
      ],
      "metadata": {
        "id": "ipGNWkTEQ1BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {Accuracy:.2f}\")\n",
        "print(f\"Precision: {Precision:.2f}\")\n",
        "print(f\"Recall: {Recall:.2f}\")\n",
        "print(f\"Specificity: {Specificity:.2f}\")\n",
        "print(f\"F1-Score: {F1_Score:.2f}\")"
      ],
      "metadata": {
        "id": "08mlUGKcRhxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_img, second_img, label = next(iter(test_dataloader))"
      ],
      "metadata": {
        "id": "vRljKmwcm5RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.transpose(first_img[1], (1, 2, 0)))"
      ],
      "metadata": {
        "id": "mNmm7AdIow2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.transpose(second_img[1], (1, 2, 0)))"
      ],
      "metadata": {
        "id": "9klFsl-GozQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_vec1, emb_vec2 = siamese_net(first_img.cuda(), second_img.cuda())\n",
        "euclidean_distance = F.pairwise_distance(emb_vec1, emb_vec2)\n",
        "\n",
        "sum_euclidean = torch.sum(euclidean_distance)\n",
        "prediction = make_prediction(sum_euclidean, label[j], 0.8)\n",
        "\n",
        "if prediction == 0:\n",
        "  out = 'Different people'\n",
        "else:\n",
        "  out = 'Same people'\n",
        "\n",
        "out"
      ],
      "metadata": {
        "id": "li_xx4gdo0BN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}